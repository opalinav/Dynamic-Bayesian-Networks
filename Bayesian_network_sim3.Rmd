---
title: "Linear model testing"
output: html_notebook
---

```{r}
#only run once if you dont have them installed
install.packages("Dict")
install.packages("bnlearn")
install.packages("data.table")
install.packages("dplyr")
install.packages("dbscan")
install.packages("data.table")
install.packages("umap")
install.packages("stats")
install.packages("bnlearn")
install.packages("tidyr")
install.packages("gplots")
install.packages("ggplot2")
install.packages("RColorBrewer")
install.packages("flifo")
install.packages("pracma")
install.packages("arules")

```

```{r}
library(Dict)
library(arules)
library(dplyr)
library(dbscan)
library(data.table)
library(umap)
library(stats)
library(bnlearn)
library(tidyr)
library(gplots)
library(ggplot2)
library(RColorBrewer)
library(flifo)
library(pracma)
library(usedist)
```

FUNCTIONS

```{r}
#set.seed(2)
#Function to sample N points from a modified gamma distribution with a nonzero
#probability of P, and shape/scale parameters given by sh and sc
Sample<-function(N=1,P=0.8,sh=2,sc=0.5)
{
  samp<-numeric(N)
  for(n in 1:N)
  {
    a<-runif(1)
    if (a>P) {samp[n]<-0}
    else {samp[n]<-rgamma(1,shape = sh,scale = sc)}
  }
return(as.matrix(samp))
}

SampleImp<-function(N=1,P=0.5,sh=2,sc=0.5)
{
  samp<-numeric(N)
  NnZ<-ceil(P*N)
  nonzerolocs<-sample(1:N,NnZ,replace = FALSE)
  samp[nonzerolocs]<-rgamma(NnZ,shape = sh,scale = sc)
  return(as.matrix(samp))
}





#Function to sample N child variable values from a gamma distribution whose 
#parameters depend on the values of P parent nodes
#Poseff-boolean length P, whether parent-child dependences are positive (removed)
#Parents - dataframe NxP composed of values for parent nodes
#effsize - numeric length P, interaction coefficients (alpha)

CondSampSingle <- function(n,Parents, Parents_medians, effsize, baseP, baseSh, baseSc) {
    Pcorr <- 1
    ScaleCorr <- 1
    for (parent in 1:length(Parents_values)) {
        b <- (Parents[n,parent] + 1) / (Parents_medians[parent] + 1)
        b <- b^effsize[parent]
        Pcorr <- Pcorr/b
        ScaleCorr <- ScaleCorr * b
    }
    return(Sample(N = 1, P = baseP^Pcorr, sh = baseSh, sc = baseSc * ScaleCorr))
}

CondSampParallel <- function(Parents, effsize = rep(1, dim(Parents)[2]), baseP = 0.8, baseSh = 2, baseSc = 0.5) {
    N <- dim(Parents)[1]
    medians <- apply(Parents, 2, median)

    if (length(effsize) != dim(Parents)[2]) {
        stop("Number of Parents must be consistent")
    } else {
        # Create a cluster with the number of cores available
        numCores <- detectCores()
        cl <- makeCluster(numCores)

        # Export the necessary variables to the cluster
        clusterExport(cl, c("Parents", "medians", "effsize", "baseP", "baseSh", "baseSc", "N"))
        
        # If Sample is a custom function, define it within the cluster.
        # Replace the body of the function in the expression below.
        clusterEvalQ(cl, {
            Sample<-function(N=1,P=0.8,sh=2,sc=0.5)
            {
              samp<-numeric(N)
              for(n in 1:N)
              {
                a<-runif(1)
                if (a>P) {samp[n]<-0}
                else {samp[n]<-rgamma(1,shape = sh,scale = sc)}
              }
            return(as.matrix(samp))
            }
         })

        # Use parLapply to parallelize the outer loop
        Res <- parLapply(cl, 1:N, function(n) {
            Pcorr <- 1
            ScaleCorr <- 1
            for (parent in 1:dim(Parents)[2]) {
                a <- Parents[n, parent]
                m <- medians[parent]
                b <- (a + 1) / (m + 1)
                b <- b^effsize[parent]
                Pcorr <- Pcorr / b
                ScaleCorr <- ScaleCorr * b
            }
            return(Sample(N = 1, P = baseP^Pcorr, sh = baseSh, sc = baseSc * ScaleCorr))
        })

        # Stop the cluster
        stopCluster(cl)

        return(as.matrix(unlist(Res)))
    }
}

CondSamp<-function(Parents,effsize=rep(1,dim(Parents)[2]),baseP=0.8,baseSh=2,baseSc=0.5)
{
  N<-dim(Parents)[1]
  medians<-numeric(length=dim(Parents)[2])
  for (parent in 1:dim(Parents)[2]){medians[parent]<-median(Parents[,parent])}
  Res<-numeric(length = N)
  #changed to length(effsize) in stead of length(Poseff)
  if(length(effsize)!=dim(Parents)[2]){stop("Number of Parents must be consistent")}
  else
  {
    for (n in 1:N)
    {
    Pcorr<-1
    ScaleCorr<-1
      for (parent in 1:dim(Parents)[2])
      {
        a<-Parents[n,parent]
        m<-medians[parent]
        #removed if statement
        b<-(a+1)/(m+1)   
        b<-b^effsize[parent]
        Pcorr<-Pcorr/b
        ScaleCorr<-ScaleCorr*b
      }
      Res[n]<-Sample(N=1,P=baseP^Pcorr,sh=baseSh,sc=baseSc*ScaleCorr)
    }
    return(as.matrix(Res))
  }
}



Disc<-function(data,quantiles)
  #Discretize with zeroes as separate bin and the rest into quantiles
{
  dat<-as.data.frame(data)
  Dis<-dat
  for (col in 1:dim(dat)[2])
  {
  d<-as.matrix(dat[,col])
  datpos<-subset(d,d>0)
  a<-arules::discretize(datpos,method="frequency",breaks=quantiles,labels=FALSE)
  count<-1
    for(n in 1:length(d))
    {
      if(d[n]==0){Dis[n,col]<-0}
      else{
        Dis[n,col]<-a[count]
        count<-count+1
      }
    }  
  }
  return(Dis)
}

discretize_var <- function(x, n_bins) {
  # Separating 0's from positive values
  zero_indices <- x == 0
  positive_values <- x[!zero_indices]
  
  # Discretizing the positive values by equal frequencies
  #cut_bins <- cut(positive_values, breaks = n_bins, labels = 1:n_bins, include.lowest = TRUE)
  cut_bins <- arules::discretize(positive_values,method="frequency",breaks=n_bins,labels=FALSE)
  # Combining 0's and discretized positive values
  result <- rep(NA, length(x))
  result[zero_indices] <- 0 # Assigning 0 values to bin "1"
  result[!zero_indices] <- cut_bins
  
  return(result)
}

DiscFast <- function(df,n_bins){
df_discretized <- as.data.frame(lapply(df, discretize_var, n_bins = n_bins))
return(df_discretized)
}


# Function to calculate correlation between two discretized variables when controlling for others
# data = discretized data frame/table whose first columns are variable values
# from, to - column numbers corresponding to the nodes whose correlation you measure
# controls - vector of column numbers corresponding to the nodes for which you need
# to control

EdgeStrength<-function(data,from,to,controls,discsize=10,samples=min(10*discsize^length(controls),50),tolerance=0.5)
{
  if (is.null(controls)){
    dat<-as.data.frame(data)[,c(from,to)]
    dat<-Disc(dat,discsize)
    return(cor(dat[,1],dat[,2]))
  }
  else{
  dat<-as.data.frame(data)[,c(from,to,controls)]
  dat<-Disc(dat,discsize)
  value<-0
  count<-0
  Nc<-length(controls)
  N<-dim(dat)[1]
  countbad<-0
  for (i in 1:samples)
  {
    if(i%%10==0){print(i)}
    if(Nc==1){x<-sample(t(dat[,3]),1)}
    else{x<-sample_n(dat[,3:(Nc+2)],1)}
    ind<-rep(FALSE,N)
    for (k in 1:N)
    { pair<-rbind(as.numeric(x),as.numeric(dat[k,3:(Nc+2)]))
      ind[k]<-(dist(pair,method = "manhattan")<=tolerance)}
    if(sum(ind)<2){countbad<-countbad+1
                    next}
    sub<-subset(dat,ind)
    n<-dim(sub)[1]
    c<-cor(sub[,1],sub[,2])
    if(!is.na(c))
    {
    count<-count+1
    value<-value+c
    print(c,value/count)
    }
    else{countbad<-countbad+1}
  }
  print(countbad)
  return(value/count)
  }
}

# Function to calculate correlation between two continuous variables when controlling for others
# data = data frame/table whose first columns are variable values. May have other 
# non-node values (e.g. well, treatment group) in later columns
# from, to - column numbers corresponding to the nodes whose correlation you measure
# controls - vector of column numbers corresponding to the nodes for which you need
# to control

EdgeStrengthCont<-function(data,from,to,controls,tolerance=0.5,samples=min(20*(1/tolerance)^length(controls),300),onlypos=FALSE)
{
  dat<-as.data.frame(data)[,c(from,to,controls)]
  if(onlypos){dat<-subset(dat,dat[,1]>0 & dat[,2]>0)}
  if (is.null(controls)){
    return(cor(dat[,1],dat[,2]))
  }
  else{
  value<-0
  count<-0
  countbad<-0
  Nc<-length(controls)
  N<-dim(dat)[1]
  for (i in 1:samples)
  {
    if(i%%10==0){print(i)}
    if(Nc==1){x<-sample(t(dat[,3]),1)}
    else{x<-sample_n(dat[,3:(Nc+2)],1)}
    sub<-dat[FALSE,]
    for (k in 1:N)
    { pair<-rbind(as.numeric(x),as.numeric(dat[k,3:(Nc+2)]))
      if(dist(pair,method = "euclidean")<=tolerance*sqrt(Nc)){sub<-rbind(sub,dat[k,])}
    }
    n<-dim(sub)[1]
    if(n<2){countbad<-countbad+1
                    next}
    c<-cor(sub[,1],sub[,2])
    if(!is.na(c))
    {
    count<-count+1
    value<-value+c
    print(c(c,value/count))
    }
    else{countbad<-countbad+1}
  }
  print(countbad)
  return(value/count)
  }
}


EdgeStrengthContFast<-function(data,from,to,controls,tolerance=0.05*sqrt(length(controls)),
                               samples=min(20*(1/tolerance)^length(controls),300),distances_provided=NULL)
{
  controlnames<-names(data)[controls]
  print(paste("Edge Strength from ", names(data)[from], " to ", names(data)[to], " with controls: ",controlnames,sep = ""))
  #If no controls just return simple Pearson correlation
  if (is.null(controls)){
    return(cor(data[,from],data[,to]))
  }
  
  #If distance matrix for controls use it, otherwise calculate
  if(is.null(distances_provided)){distances<-dist(data[,controls])}
  else{distances<-distances_provided}
  print("distance matrix created")
  
  value<-0
  count<-0
  countbad<-0
  N<-dim(data)[1]
  for (i in 1:samples){ #sample points from dataset randomly, for each one create subset of close points, calculate correlation in subset
    samprow<-sample(1:N,1)
    withintol<-(dist_get(distances,samprow,1:N)<tolerance) #take only points whose control values are within tolerance of sampled point
    if(sum(withintol)<10){countbad<-countbad+1
                    next}
    sub<-data[withintol,]
    c<-cor(sub[,from],sub[,to])
    if(!is.na(c))
    {
      count<-count+1
      value<-value+c
      #print(c(c,value/count))
    }
    else  {countbad<-countbad+1}
  }
  print(paste(countbad, "bad samplings out of",samples))
  return(value/count)
}


# Function to estimate the effect
# data = data frame/table whose first columns are variable values. May have other 
# non-node values (e.g. well, treatment group) in later columns
# from, to - column numbers corresponding to the nodes whose correlation you measure
# controls - vector of column numbers corresponding to the nodes for which you need
# to control

TreatStrengthCont<-function(data,Treats,target,controls,Compare="NT",tolerance=0.5,
                            samples=min(10*(1/tolerance)^length(controls),200),onlypos=controls)
{
  Nc<-length(controls)
  dat1<-as.data.frame(subset(data,data$Trt%in%Treats))
  dat2<-as.data.frame(subset(data,data$Trt==Compare))
  
  if(!is.null(onlypos)){
    for (con in onlypos)
    { dat1<-subset(dat1,dat1[,con]>0)
      dat2<-subset(dat2,dat2[,con]>0)}
  }
  dat1<-dat1[,c(target,controls)]
  dat2<-dat2[,c(target,controls)]
  if (is.null(controls)){
    m1<-mean(as.matrix(dat1)[,1])
    m2<-mean(as.matrix(dat2)[,1])
    return(log(m1/m2,base = 2))
  }
  else{
  value<-0
  count<-0
  countbad<-0
  N1<-dim(dat1)[1]
  N2<-dim(dat2)[1]
  for (i in 1:samples)
  {
    if(Nc==1){x<-sample(t(dat1[,2]),1)}
    else{x<-sample_n(dat1[,2:(Nc+1)],1)}
    sub1<-dat1[FALSE,]
    sub2<-dat2[FALSE,]
    for (k in 1:N1)
    { pair<-rbind(as.numeric(x),as.numeric(dat1[k,2:(Nc+1)]))
      if(dist(pair,method = "euclidean")<=tolerance*sqrt(Nc)){sub1<-rbind(sub1,dat1[k,])}
    }
    for (k in 1:N2)
    { pair<-rbind(as.numeric(x),as.numeric(dat2[k,2:(Nc+1)]))
      if(dist(pair,method = "euclidean")<=tolerance*sqrt(Nc)){sub2<-rbind(sub2,dat2[k,])}
    }
    n1<-dim(sub1)[1]
    n2<-dim(sub2)[1]
    m1<-mean(as.matrix(sub1)[,1])
    m2<-mean(as.matrix(sub2)[,1])
    if(m1*m2==0 | is.na(m1/m2)){countbad<-countbad+1
                    next}
    else
    {
    count<-count+1
    value<-value+log(m1/m2,2)
    print(paste(signif(log(m1/m2,2),4), signif(value/count,4), sep = " _ "))
    }
    if(i%%10==0){print(i)}
  }
  if(countbad>0.75*samples){print(paste("Error: trouble with treatment",Treat,"and target", target, "Countbad=", countbad,sep = " "))}
  return(value/count)
  }
}

#Calculates conditional treatment effect on individual wells

TreatWellCont<-function(data,Wells,target,controls,Compare="NT",tolerance=0.5,
                            samples=min(10*(1/tolerance)^length(controls),200),onlypos=controls,samplecompare=10000)
{
  Nc<-length(controls)
  dat1<-as.data.frame(subset(data,data$Well%in%Wells))
  dat2<-as.data.frame(subset(data,data$Trt==Compare))
  dat2<-sample_n(dat2,samplecompare,replace = TRUE)
  
  if(!is.null(onlypos)){
    for (con in onlypos)
    { dat1<-subset(dat1,dat1[,con]>0)
      dat2<-subset(dat2,dat2[,con]>0)}
  }
  dat1<-dat1[,c(target,controls)]
  dat2<-dat2[,c(target,controls)]
  if (is.null(controls)){
    m1<-mean(as.matrix(dat1)[,1])
    m2<-mean(as.matrix(dat2)[,1])
    return(log(m1/m2,base = 2))
  }
  else{
  value<-0
  count<-0
  countbad<-0
  N1<-dim(dat1)[1]
  N2<-dim(dat2)[1]
  for (i in 1:samples)
  {
    if(Nc==1){x<-sample(t(dat1[,2]),1)}
    else{x<-sample_n(dat1[,2:(Nc+1)],1)}
    sub1<-dat1[FALSE,]
    sub2<-dat2[FALSE,]
    for (k in 1:N1)
    { pair<-rbind(as.numeric(x),as.numeric(dat1[k,2:(Nc+1)]))
      if(dist(pair,method = "euclidean")<=tolerance*sqrt(Nc)){sub1<-rbind(sub1,dat1[k,])}
    }
    for (k in 1:N2)
    { pair<-rbind(as.numeric(x),as.numeric(dat2[k,2:(Nc+1)]))
      if(dist(pair,method = "euclidean")<=tolerance*sqrt(Nc)){sub2<-rbind(sub2,dat2[k,])}
    }
    n1<-dim(sub1)[1]
    n2<-dim(sub2)[1]
    m1<-mean(as.matrix(sub1)[,1])
    m2<-mean(as.matrix(sub2)[,1])
    if(m1*m2==0 | is.na(m1/m2)){countbad<-countbad+1
                    next}
    else
    {
    count<-count+1
    value<-value+log(m1/m2,2)
    print(paste(signif(log(m1/m2,2),4), signif(value/count,4), sep = " _ "))
    }
    if(i%%10==0){print(i)}
  }
  if(countbad>0.75*samples){print(paste("Error: trouble with well",Well,"and target", target, "Countbad=", countbad,sep = " "))}
  return(value/count)
  }
}

#Takes bootstrapping result (i.e. output from boot.strength) as "network", parent
# node, child node, and returns numbers corresponding to all other parents
#of child node

ReturnParents<-function(from,to,network,
                        names=c("Synapsin","GluR2","Homer","PSD95","Shank","Actin","Bassoon","vGlut"),
                        strcutoff=0.8,dircutoff=0.6)
{
  network<-subset(network,network$strength>=strcutoff)
  Fromnums<-mapvalues(network$from,names,1:length(names))
  Tonums<-mapvalues(network$to,names,1:length(names))
  Nedges<-length(Fromnums)
  
  loc<-which(Fromnums==from & Tonums==to)
  if(length(loc)==0){return(0)}
  else if(length(loc)>1){stop("Error:edge repeats twice")}
  else
  {
    parents<-which(Tonums==to & Fromnums!=from & network$direction>1-dircutoff)
    if (network$direction[loc]<dircutoff) {parents<-c(parents,which(Tonums==from & Fromnums!=to & network$direction>1-dircutoff))}
  }
  return(as.numeric(levels(factor(Fromnums[parents]))))
}

#Automatically calculates the strengths of all network edges in a data subset

AllEdgeStrengths<-function(data,network,N = 8,names,strcutoff,dircutoff)
{
  RawEdges<-matrix(0, nrow = N, ncol = N)
  ContEdges<-matrix(0, nrow = N, ncol = N)
  
  for (row in 1:N)
  {
    for (col in 1:N)
    {
      print(c(row,col))
      p<-ReturnParents(row,col,network,names,strcutoff,dircutoff)
      if(p[1]!=0)
      {
        RawEdges[row,col]<-cor(data[,..row],data[,..col])
        ContEdges[row,col]<-EdgeStrengthCont(data,from = row,to = col,controls = p)
      }
    }
  }
  return(list(RawEdges,ContEdges))
}

#Returns all well names from a certain treatment group
TrtGroup<-function(well,data)
{x<-levels(factor(subset(data$Trt,data$Well==well)))
  if(length(x)!=1){stop("ERROR")}
  else return(x)}

#Creates custom color map
CreateColorMap<-function(colors,breaks,data,char=TRUE)
{
  N<-length(data)
  M<-length(breaks)
  if(char){col=character(N)}
  else{col=numeric(N)}
  for (i in 1:N)
  {if(breaks[1]>data[i]){wh<-1}
   else {wh<-max(which(breaks<data[i]))}
   col[i]<-colors[wh+1]}
  return(col)
}

#Calculates  mutual information between X and Y conditional on conts
ContMutInf<-function(data,X,Y,conts)
{return(entropy(data[,c(X,conts)])+entropy(data[,c(Y,conts)])-entropy(data[,c(X,Y,conts)])-entropy(data[,conts]))}


#Function to progress an N-variable distribution by a certain number of noisy 
#linear steps with a constant input
#X(t+1)=(1+nsO)*[M*(1+nsM)*X(t)*(1+nsX)+input]
#M is the NxN interaction matrix (x(t+1)=M*x(t))
#dat is the distribution (data frame with N columns)
#steps is number of steps to propagate
#noisein, noiseout, noiseM are sd of gaussian with center 1 to multiply input,
#output and matrix

Linstep<-function(M,dat,steps=1,noisein=0,noiseout=0,noiseM=0,input=0)
{
  if(steps==1)
  {
  inp<-input*abs(rnorm(length(input),mean = 1,sd=noisein))
  Mn<-M*abs(rnorm(prod(dim(M)),mean = 1,sd=noiseM))
  b<-apply(dat,MARGIN = 1,FUN = function(v){M%*%v+inp})
  b<-as.data.frame(t(b))
  names(b)<-names(dat)
  N<-prod(dim(dat))
  b<-b*abs(rnorm(N,mean = 1,sd = noiseout))
  return(b)
  }
  else
  {
  updat<-Linstep(M,dat,steps=1,noisein=noisein,noiseout = noiseout,noiseM = noiseM,input = input)
  return(Linstep(M,updat,steps=steps-1,noisein=noisein,noiseout = noiseout,noiseM = noiseM,input = input))
  }
}

#Functions to test similarity of distributions:

#tests the probability of a certain point against distributions defined by dat
#of gaussians. Each gaussians sigma is stdev of underlying points divided by
#precfactor
TestPointGauss<-function(dat,point,precfactor,sigmas=apply(dat,2,sd)/precfactor,initrestrict=5)
{
  if(dim(dat)[2]!=length(point)){stop("Point doesn't match dimensions of data")}
  P<-0
  for (row in 1:dim(dat)[1])
  {
    pr<-1
    for (col in 1:dim(dat)[2])
    {
      s<-sigmas[col]/precfactor
      x<-((dat[row,col]-point[col])/s)^2
      if(x>initrestrict^2){pr<-0
                            break}
      else{pr<-pr*exp(-0.5*x)/(s*2.50663)}
    }
    P<-P+pr
  }
  return(P/dim(dat)[1])
}

TestPointNH<-function(dat,point,eps)
{
  if(dim(dat)[2]!=length(point)){stop("Point doesn't match dimensions of data")}
  N<-dim(dat)[1]
  pointrep<-matrix(rep(point,each=N),nrow = N)
  dif<-dat-pointrep
  difnorm<-apply(dif,1,function(x){Norm(x,p=2)})
  count<-sum(difnorm<eps)
  return((count+1)/N)
}

TestSetNH<-function(dat1,dat2,sub1 = 100, sub2 = "All", eps=0.5)
{
  if(sub1=="All"){d1<-dat1
                  N<-dim(dat1)[1]}
  else {d1<-sample_n(dat1,sub1)
        N<-sub1}
  if(sub2=="All"){d2<-dat2}
  else {d2<-sample_n(dat2,sub2)}
  
  sumlogs<-0
  
  for (i in 1:N)
  {
    p<-TestPointNH(d2,as.numeric(d1[i,]),eps)
    sumlogs<-sumlogs-log(p)
  }
  return(sumlogs/sub1)
}


```

More Functions:

1.  Create Net
2.  Sensitivity
3.  Scoring
4.  Generate Matrices
5.  Heat Map testing Scaling Laws


```{r}

##Create simulated network given matrice
create_net<-function(matrix, N){
  #changed vec from list to data frame
  vec<-data.frame(V1=rep(0,N))
  all_parents<-list()
  dimensions <- dim(matrix)
  for (rows in 1:dimensions[1]) {
    #if row is all zero, just use sample (it is an independent node)
    row_name<-rownames(matrix)[rows]
    if (identical(matrix[rows,],rep(0,dimensions[2]))){
      A<-as.data.frame(SampleImp(N=N))
      print(paste("Node", row_name, "no parents, finished"))
      #else it is a dependent node, need to figure out parents and eff_sizes
    } else{
      parents<-list()
      eff_size<-c()
      i<-1
      for (columns in 1:dimensions[2]) {
        if (matrix[rows,columns]!=0){
          #figure out relevant parents of current node
          parents[[i]]<-all_parents[[columns]]
          i<-i+1
          eff_size<-append(eff_size, as.numeric(matrix[rows,columns]))
          #print(as.numeric(matrix[rows,columns]))
        }
      }
      parents<-do.call(cbind, parents)
      #define node that has dependencies using CondSamp
      A<-as.data.frame(CondSamp(parents,effsize = eff_size))
      print(paste("Node", row_name, length(eff_size), "parents, finished"))
    }
    all_parents[[rows]]<-A
    vec<-cbind(vec,A)
  }
  #outside for-loop construct data table with nodes
  #nodes<-do.call(cbind, vec)
  vec<-vec[,-1]
  dat<-vec#as.data.table(nodes)
  names(dat)<-rownames(matrix)
  return(dat)
  
}


#Sensitivity Function

sensitivity<-function(matrix,dims,range){
  matrix2<-matrix
  vec<-data.frame()
  for (eff in range){
    print(eff)
    for (row_ in seq(1,(length(dims)-1),2)){

      col_<-dims[row_+1]
      matrix2[dims[row_],col_]<-eff

    }

    #matrix2[row_,col_]<-eff
    dat<-create_net(matrix2,10000)
    d<-Disc(dat,200)
    BN<-tabu(d)
    Bass<-boot.strength(dat,R=500,m=3000,algorithm = "tabu")
    node<-rownames(matrix2)[dims[1]]
   
    #only care about rows relating to changing node
    bass_select<-Bass[Bass$to==node | Bass$from==node,]
    num_rows<-nrow(bass_select)
    names<-rownames(matrix2)
    eff_col<-rep(eff,num_rows)
    #bass_select[matrix2[which(names==bass_select$to)][which(names==bass_select$from)],]
    bass_select<-cbind(eff_col, bass_select)
    if (nrow(vec)==0){
      vec<-bass_select
    }
    else{
      vec<-rbind(vec, bass_select)
    }
  }
  return(vec)
  
}

#have same matrix with differing number of data points, nodes x axis, datapoints y -axis, score (color in heatmap)
score<-function(bootstrap, mat,data, tolerance){
  data<-as.data.frame(scale(data,center=TRUE))
  sum_edges<-0
  sum_dirs<-0
  cap<-toupper(letters)
  for(i in 1:nrow(bootstrap['strength'])) {
    #turn to and from nodes from letters to numbers, ex. A->1, B->2, etc. 
    to<-which(cap==bootstrap['to'][i,])
    from<-which(cap==bootstrap['from'][i,])
    #edge=0 if no edge between to and from in original matrix, or 1 if node
    edge<-mat[to,from]>0 || mat[from,to] >0
    
    #dir=0 if wrong dir (not an edge from B to C), and 1 if right if correct direction (from C to B)
    #only add dir, if there is an edge there
    if (edge==1){
      dir<- mat[to, from]>0
      direct<-bootstrap['direction'][i,]
      edgestrength<-mat[to,from]+mat[from,to]
    }
    else{
      dir<- 0
      direct<-0
      edgestrength<-1
    }
    strength<-bootstrap['strength'][i,]
    controls<-seq(1,nrow(mat))
    controls<-controls[-c(to,from)]
    
    weight<-EdgeStrengthContFast(data,to,from,controls, tolerance=tolerance)
    
    #add cost object of deviation from edge presence and direction
    #if edge exists weigh by its strength
    sum_edges<-sum_edges+weight*abs(strength-edge)
    sum_dirs<-sum_dirs+weight*abs(dir-direct)
  }
  N<-dim(mat)[1]
  return(c(sum_edges/(N^2),sum_dirs/(N^2)))
}

#Generating Matrices
contains_isolated_node<-function(matrix){
  N<-dim(matrix)[1]
  vec<-numeric(N)
  for(i in 1:N){vec[i]<-sum(matrix[i,]+matrix[,i])}
  return((prod(vec)==0))
}


gen_matrice<-function(N,rate=4){
  cap<-toupper(letters)
  mat<-matrix(data=0, nrow=N, ncol=N)
  while(contains_isolated_node(mat)){ #checking for isolated nodes
  for (r in 2:N){
    for (c in 1:(r-1)){
      #1 if there is an edge, 0 if no edge
      edge<-rbinom(n = 1, prob = 0.5, size = 1)
      if (edge){
        mat[r,c]<-rexp(1, rate)
      }
      else{
        mat[r,c]<-0
      }
    }
  }
  }
  rownames(mat)<-cap[1:N]
  return(mat)
}


#Heat Map on Simulated Data
simulation_heatmap<-function(datapoints,nodes){
  x<-c()
  y<-c()
  z<-c()
  
  for (point in datapoints){
    for (node in nodes){
      a<-gen_matrice(node)
      ctime<-system.time(dat<-create_net(a,point))
      dtime<-system.time(d<-DiscFast(dat,point/200))
      btime<-system.time(Bass<-boot.strength(d,R=20,m=point/10,algorithm = "tabu"))
      stime<-system.time(score_<-score(Bass,a))
      x<-append(x,node)
      y<-append(y,point)
      z<-append(z,score_)
      
      print(point+node)
      print(ctime)
      print(dtime)
      print(btime)
      print(stime)
    }
  }
  #data <- expand.grid(X=x, Y=y)
  #data<-cbind(X=x, Y=y,Z=z)
  data<-data.frame(X=x, Y=y, Z=z)
  return(data)
}

combine_linsteps<-function(time_steps,matrix,data){
  dat_all<-data.table()
  time<-rep(time_steps,each=nrow(data))
  for (t in time_steps){
    dat<-Linstep(matrix,data, t)
    dat_all<-rbind(dat_all,dat)
    
    
  }
  #dat_all<-Disc(dat_all,200)
  dat_all$Time<-time
  
  return(dat_all)
}



```

Testing functions

1.  Testing Create Net

```{#create_net example}
A<-c(0,0,0,0)
B<-c(0,0,0,0)
C<-c(.3,-.1,0,0)
D<-c(1,0,0,0)
mat<-rbind(A,B,C,D)

dat<-create_net(mat,10000)
```

2.  Testing Sensitivity

```{r}
A<-c(0,0,0,0,0,0,0,0)
B<-c(0,0,0,0,0,0,0,0)
C<-c(.5,0,0,0,0,0,0,0)
D<-c(0,.6,.6,0,0,0,0,0)
E<-c(0,.5,0,0,0,0,0,0)
F<-c(0,0,0,.45,.55,0,0,0)
G<-c(0,0,0,0,.75,0,0,0)
H<-c(0,0,0,0,0,0,0,0)
mat<-rbind(A,B,C,D,E,F,G,H)
range<-c(seq(.01,.1,.005),seq(.1,.3,.025))
sense<-sensitivity(mat,6,4, range)
print(sense)

#see if BC decreases as AC and CD get stronger
A<-c(0,0,0,0)
B<-c(.05,0,0,0)
C<-c(0,.05,0,0)
D<-c(0,0,0,0)
mat<-rbind(A,B,C,D)
sense<-sensitivity(mat, c(3,1,4,3), seq(.1,1,.1))

print(sense)

```

SCORING FUNCTION

```{r}
A<-c(0,0,0,0,0,0,0,0)
B<-c(0,0,0,0,0,0,0,0)
C<-c(.5,0,0,0,0,0,0,0)
D<-c(0,.6,.6,0,0,0,0,0)
E<-c(0,.5,0,0,0,0,0,0)
F<-c(0,0,0,.45,.55,0,0,0)
G<-c(0,0,0,0,.75,0,0,0)
H<-c(0,0,0,0,0,0,0,0)
mat<-rbind(A,B,C,D,E,F,G,H)
mat
dat<-create_net(mat,10000)
#dat_std<-as.data.frame(scale(dat,center=True))

d<-Disc(dat_std,200)
BN<-tabu(d)
plot(BN)
Bass<-boot.strength(d,R=500,m=3000,algorithm = "tabu")
print(Bass)

print(score(Bass,mat,dat,tolerance=.5))

```

MATRICE GENERATOR

```{r}

a<-gen_matrice(16)
dat<-create_net(a,4096000)
d<-Disc(dat,40)

```

HEAT-MAP

```{r}
#y=number of data points, x=# of nodes, color=quality
datapoints<-c(2000,4000,8000,16000,32000,64000,128000,256000,512000,1024000,2048000,4096000)
nodes<-seq(6,16,2)
repeats<-list()
for(i in 1:20){
print(paste("Thus starts repeat", i))
repeats[[i]]<-simulation_heatmap(datapoints,nodes)
}

sum<-numeric(length(datapoints)*length(nodes))
for (i in 1:20){sum<-sum+repeats[[i]]$Z}
m<-matrix(data=sum,nrow = length(nodes),ncol = length(datapoints))
rownames(m)<-nodes
colnames(m)<-datapoints
M<-m

heatmap.2(M,Rowv = FALSE, Colv = FALSE, dendrogram = "none",trace = "none")

```

```{r}
#Creating Simulated data
nodes<-seq(6,16,2)
for (node in 16){
  for (rep in 3:20){
    print(paste("Making SimData_",node,"nodes_rep_",rep,".csv",sep = ""))
    mat<-gen_matrice(node)
    filename<-paste("D:/Large Simulated Datasets/EdgeProb-0-5-exp-rate-4-20repeats-6-16-nodes-4Mpoints/SimData_",node,"_nodes_rep_",rep,".csv",sep = "")
    filenamematrix<-paste("D:/Large Simulated Datasets/EdgeProb-0-5-exp-rate-4-20repeats-6-16-nodes-4Mpoints/Matrix_",node,"_nodes_rep_",rep,".csv",sep = "")
    write.csv(mat,filenamematrix)
    dat<-create_net(mat,4096000)
    fwrite(dat,filename)
  }
}
```

```{r}
set.seed(42)
datapoints<-c(2000,4000,8000,16000,32000,64000,128000,256000,512000,1024000,2048000,4096000)
nodes<-seq(6,16,2)
Hmaps_edges<-list()
Hmaps_dirs<-list()
path<-"D:/Large Simulated Datasets/EdgeProb-0-5-exp-rate-4-20repeats-6-16-nodes-4Mpoints/"
for (rep in 1:20){
  Hmaps<-matrix(0,length(nodes),length(datapoints))
  rownames(Hmaps)<-nodes
  colnames(Hmaps)<-datapoints
  Hmaps_edges[[rep]]<-Hmaps
  Hmaps_dirs[[rep]]<-Hmaps
  for (node in nodes){
    filenameM<-paste("Matrix_",node,"_nodes_rep_",rep,".csv",sep = "")
    filenameD<-paste("SimData_",node,"_nodes_rep_",rep,".csv",sep = "")
    print(filenameM)
    for (datapoint in datapoints){
      print(datapoint+node)
      mat<-read.csv(paste(path,"Simulated Data/",filenameM,sep = ""))
      mat<-as.matrix(mat[,-1])
      ctime<-system.time(dat<-fread(paste(path,"Simulated Data/",filenameD,sep = "")))
      samp<-sample_n(dat,datapoint,replace = FALSE)
      dtime<-system.time(d<-DiscFast(samp,100))
      btime<-system.time(Bass<-boot.strength(d,R=20,m=datapoint/10,algorithm = "iamb"))
      stime<-system.time(score_<-score(Bass,mat))
      print(ctime)
      print(dtime)
      print(btime)
      print(stime)
      Hmaps_edges[[rep]][which(nodes==node),which(datapoints==datapoint)]<-score_[1]
      Hmaps_dirs[[rep]][which(nodes==node),which(datapoints==datapoint)]<-score_[2]
    }
  }
write.csv(Hmaps_edges[[rep]],paste(path,"Network Scoring results_iamb/EdgeScores_",rep,".csv",sep=""))
write.csv(Hmaps_dirs[[rep]],paste(path,"Network Scoring results_iamb/DirScores_",rep,".csv",sep=""))
}

```

GRAPHING FUNCTION

```{r}
set.seed(8)
create_graph<-function(sensitivity){
  
  y<-sensitivity[sensitivity$to=='B' & sensitivity$from=='C',]

  graph<-ggplot()+geom_point(data=y, aes(x=eff_col, y=strength)) +ggtitle("B to C strength as AC and CD increase")
  
}
A<-c(0,0,0,0)
B<-c(.3,0,0,0)
C<-c(0,.05,0,0)
D<-c(0,0,0,0)
mat<-rbind(A,B,C,D)
sense1<-sensitivity(mat, c(3,1,4,3), seq(.5,1,.025))
mat[3,2]<-.1
sense2<-sensitivity(mat, c(3,1,4,3), seq(.5,1,.025))

g1<-create_graph(sense1)+ggtitle("B to C strength as AC and CD increase, BC=.05")
g2<-create_graph(sense2)+ggtitle("B to C strength as AC and CD increase, BC=.1")

g1
g2

```

EXAMPLES OF SIMULATED STATIC NETWORKS AND NETWORK INFERENCE

```{r}
#Create Network that is A,B,C|(AB),D|A
set.seed(4)

A<-Sample(N=10000)

B<-Sample(N=10000)
C<-CondSamp(c(TRUE,FALSE),cbind(A,B),effsize = c(0.3,0.1))

D<-CondSamp(TRUE,as.matrix(A)) 
a<-Disc(A,200)
b<-Disc(B,200)
c<-Disc(C,200)
d<-Disc(D,200)

dat<-as.data.table(cbind(a,b,c,d))
names(dat)<-c("A", "B","C","D")
head(dat)
BN<-tabu(dat)
plot(BN)

Bass<-boot.strength(dat,R=500,m=3000,algorithm = "tabu")
print(Bass)
#Making network: A, B, C|A, D, E|(D,C), F|(B,D)

A<-Sample(N=10000)
B<-Sample(N=10000)
C<-CondSamp(TRUE,as.matrix(A))
D<-Sample(N=10000)
E<-CondSamp(c(TRUE,TRUE),cbind(C,D),effsize = c(0.2,1))
G<-CondSamp(c(TRUE,TRUE),cbind(B,D),effsize = c(1,1))
a<-Disc(A,20)
b<-Disc(B,20)
c<-Disc(C,20)
d<-Disc(D,20)
e<-Disc(E,20)
g<-Disc(G,20)
dat<-as.data.table(cbind(a,b,c,d,e,g))
#runs network on data once, can print
BN<-tabu(dat)
#boot strength applies algorithm 500 times on samples of size 3000, outputs average of networks
Bass<-boot.strength(dat,R=500,m=3000,algorithm = "tabu")
```

```{r}
#Making network: A, B, C|(A,B), D|B, E|(J,C,D), G|(B,E), H|(E,C,J), I|(B,D,G) ,J|C
al<-1
A<-Sample(N=20000)
B<-Sample(N=20000)
C<-CondSamp(cbind(A,B),effsize = c(1*al,0.2*al))
D<-CondSamp(cbind(B),effsize = c(0.5*al))
J<-CondSamp(cbind(C),effsize = c(0.6*al))
E<-CondSamp(cbind(C,D,J),effsize = c(0.8*al,1*al,0.7*al))
G<-CondSamp(cbind(E,B),effsize = c(0.5*al,0.4*al))
H<-CondSamp(cbind(C,E,J),effsize = c(0.2*al,0.4*al,1*al))
I<-CondSamp(cbind(B,D,G),effsize = c(0.1*al,0.4*al,1*al))

vars<-c("A","B","C","D","E","G","H","I","J")

#simulated data 
dat<-cbind(A,B,C,D,E,G,H,I,J)
data<-as.data.frame(dat)#return this 

#infer a network from simulated data
names(data)<-vars
#discretize
dat<-as.data.frame(Disc(dat,20))
names(dat)<-vars
ordat<-dat
ordat[]<-lapply(ordat,as.ordered)
names(ordat)<-vars

#applying the network inference algorithm, tabu worked best
iamb1<-boot.strength(ordat,R=200,m=500,algorithm = "iamb")
interiamb1<-boot.strength(ordat,R=200,m=500,algorithm = "inter.iamb")
fdr1<-boot.strength(ordat,R=200,m=500,algorithm = "iamb.fdr")
mmpc1<-boot.strength(dat,R=500,m=3000,algorithm = "mmpc")
tabu2<-boot.strength(dat,R=500,m=3000,algorithm = "tabu")
mmhc1<-boot.strength(dat,R=500,m=3000,algorithm = "mmhc")
h2pc1<-boot.strength(dat,R=500,m=3000,algorithm = "h2pc")
#EdgeStrengthCont<-function(data,from,to,controls,tolerance=0.5,samples=min(20*(1/tolerance)^length(controls),300),onlypos=FALSE)
#AC_B<-EdgeStrengthContFast(data,1,3,2)
BC_A<-EdgeStrengthCont(data,2,3,1)
CJ<-EdgeStrengthCont(data,3,9,NULL)
CE_DJ<-EdgeStrengthCont(data,3,5,c(4,9))
DE_CJ<-EdgeStrengthCont(data,4,5,c(3,9))
JE_CD<-EdgeStrengthCont(data,9,5,c(3,4))
CH_EJ<-EdgeStrengthCont(data,3,7,c(5,9))
EH_CJ<-EdgeStrengthCont(data,5,7,c(3,9))
JH_CE<-EdgeStrengthCont(data,9,7,c(3,5))
BD<-EdgeStrengthCont(data,2,4,NULL)
BG_E<-EdgeStrengthCont(data,2,6,5)
EG_B<-EdgeStrengthCont(data,5,6,2)
BI_DG<-EdgeStrengthCont(data,2,8,c(4,6))
DI_BG<-EdgeStrengthCont(data,4,8,c(2,6))
GI_BD<-EdgeStrengthCont(data,6,8,c(2,4))



```

```{r}
#Making network with cycles: A, B|(A,H),C|(A,D),D|(E,B),E|C ,G|D, H|G, I|E,G

A<-Sample(N=20000)
B1<-CondSamp(c(TRUE),cbind(A),effsize = c(0.1*al))
C1<-CondSamp(c(TRUE),cbind(A),effsize = c(0.7*al))
E<-CondSamp(c(TRUE),cbind(C1),effsize = c(0.8*al))
D<-CondSamp(c(TRUE,TRUE),cbind(B1,E),effsize = c(1*al,0.6*al))
C2<-CondSamp(c(TRUE),cbind(D),effsize = c(0.4*al))
C<-0.5*(C1+C2)
G<-CondSamp(c(TRUE),cbind(D),effsize = c(0.3*al))
H<-CondSamp(c(TRUE),cbind(G),effsize = c(0.8*al))
B2<-CondSamp(c(TRUE),cbind(H),effsize = c(0.3*al))
B<-0.5*(B1+B2)
I<-CondSamp(c(TRUE,TRUE),cbind(E,G),effsize = c(0.1*al,0.4*al))

vars<-c("A","B","C","D","E","G","H","I")

dat<-cbind(A,B,C,D,E,G,H,I)
data<-as.data.frame(dat)
names(data)<-vars
dat<-as.data.frame(Disc(dat,20))
names(dat)<-vars
ordat<-dat
ordat[]<-lapply(ordat,as.ordered)
names(ordat)<-vars

tabu1<-boot.strength(dat,R=500,m=3000,algorithm = "tabu")

CA<-EdgeStrengthCont(data,1,3,NULL)
DC_E<-EdgeStrengthCont(data,4,3,5)
EC_D<-EdgeStrengthCont(data,5,3,4)
ED_B<-EdgeStrengthCont(data,5,4,2)
BD_E<-EdgeStrengthCont(data,2,4,5)
BH<-EdgeStrengthCont(data,2,7,NULL)
DG_H<-EdgeStrengthCont(data,4,6,7)
HG_D<-EdgeStrengthCont(data,7,6,4)
GI<-EdgeStrengthCont(data,6,8,NULL)

```



CELL RANK MATRIX
```{r}
###used this for creating data for cellrank
set.seed(8)
#Create a simulated dynamic network of A, B|A, C|B, D|C, E|B,C by starting with
#independent variables and propagating

A<-SampleImp(N=5000,P=.9,sh=.2)
B<-SampleImp(N=5000,P=.9,sh=0.2)
C<-SampleImp(N=5000,P=.9,sh=0.2)
D<-SampleImp(N=5000,P=.9,sh=0.2)
E<-SampleImp(N=5000,P=.9,sh=0.2)
F1<-SampleImp(N=5000,P=.9,sh=.2)
G<-SampleImp(N=5000,P=.9,sh=0.2)
H<-SampleImp(N=5000,P=.9,sh=0.2)
I<-SampleImp(N=5000,P=.9,sh=0.2)
J<-SampleImp(N=5000,P=.9,sh=0.2)
dat1<-as.data.frame(cbind(A,B,C,D,E,F1,G,H,I,J))
names(dat1)<-c("A","B","C","D","E", "F1", 'G', 'H','I', 'J')
# Create vectors vecA through vecJ with length 10 and set all diagonals to 0.8
vecA <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
vecB <- c(.2, 0.8, 0, 0, 0, 0, 0, 0, 0, 0)
vecC <- c(.3, .1, 0.6, 0, 0, 0, 0, 0, 0, 0)
vecD <- c(0, .3, .2, 0.5, 0, 0, 0, 0, 0, 0)
vecE <- c(.1, 0, .4, 0, 0.5, 0, 0, 0, 0, 0)
vecF <- c(0, 0, 0, 0, .4, 0.6, 0, 0, 0, 0)
vecG <- c(0, 0, .2, 0, .1, .1, 0.6, 0, 0, 0)
vecH <- c(.5, 0, 0, .2, 0, .1, 0, 0.2, 0, 0)
vecI <- c(0, 0, 0, .3, .2, 0, .1, 0, 0.4, 0)
vecJ <- c(0, 0, 0, 0, .1, 0, 0, .5, 0, 0.4)



# Merge the vectors into a matrix
merged_matrix <- t(matrix(cbind(vecA, vecB, vecC, vecD, vecE, vecF, vecG, vecH, vecI, vecJ), ,nrow = 10,ncol=10))




#c(5,10,15,20,25,30,35,40)
dat_new<-combine_linsteps(c(5,10,15,20,25,30,35,40),merged_matrix,dat1)
write.csv(dat_new, 'linstep_data2.csv')

#BN<-tabu(dat_new)
#plot(BN)
#Bass<-boot.strength(dat_new,R=500,m=3000,algorithm = "tabu")
#print(Bass)
library(dplyr)

# Group by timestep and calculate the average for each protein
averages <- dat_new %>%
  group_by(Time) %>%
  summarise(
    Avg_A = mean(A),
    Avg_B = mean(B),
    Avg_C = mean(C),
    Avg_D = mean(D),
    Avg_E = mean(E),
    Avg_F1 = mean(F1),
    Avg_G = mean(G),
    Avg_H = mean(H),
    Avg_I =mean(I),
    Avg_J=mean(J)
  )

# Print the result

print(averages)


```

```{r}



library(ggplot2)

# Reshape the dataframe to long format
library(tidyr)
df_long <- gather(dat_new, key = "Protein", value = "Value", -Time)

# Create a ggplot object with facets for each protein
ggplot(df_long, aes(x = Time, y = Value)) +
  
  # Add a geom_point layer for individual data points
  geom_point(alpha = 0.5) +
  
  # Add a geom_line layer for connecting points (optional)
  geom_line(stat = "summary", fun = "mean", color = "blue", size = 1) +
  
  # Facet by Protein to create separate plots for each protein
  facet_wrap(~Protein, scales = "free_y") +
  
  # Add labels and title
  labs(
    x = "Time",
    y = "Average Protein Value",
    title = "Average Protein Values at Different Time Points"
  )


```


```{r}
# Assuming df is your data frame with columns "A" through "J" and "Time"

# Install and load the ggplot2 package if not already installed
# install.packages("ggplot2")
library(ggplot2)

# Reshape the dataframe to long format
library(tidyr)
df_long <- gather(dat_new, key = "Protein", value = "Value", -Time)

# Create a ggplot object with facets for each protein
ggplot(df_long, aes(x = Time, y = Value)) +
  
  # Add a geom_point layer for individual data points
  geom_point(alpha = 0.5) +
  
  # Add a geom_line layer for connecting points (optional)
  geom_line(stat = "summary", fun = "mean", color = "blue", size = 1) +
  
  # Facet by Protein to create separate plots for each protein
  facet_wrap(~Protein, scales = "free_y") +
  
  # Add labels and title
  labs(
    x = "Time",
    y = "Average Protein Value",
    title = "Average Protein Values at Different Time Points"
  )

```




```{r}
set.seed(4)
#changing P doesn't change anything
A<-Sample(N=10000,P=.9,sh=3)
B<-Sample(N=10000,P=.9,sh=2)
C<-Sample(N=10000,P=.9,sh=.5)
D<-Sample(N=10000,P=.9,sh=0.2)
E<-Sample(N=10000,P=.9,sh=0.2)
dat2<-as.data.frame(cbind(A,B,C,D,E))
names(dat2)<-c("A","B","C","D","E")
M2<-t(matrix(c(0.8,0,0,0,0,0.9,0.8,0,0,0,0,0.4,0.8,0,0,0,0,0.2,0.8,0,0,0.8,0.8,0,0.8),nrow = 5,ncol=5))
dat_new<-combine_linsteps(c(5,10,15,20),M2,dat2)


BN<-tabu(dat_new)
plot(BN)
Bass<-boot.strength(dat_new,R=500,m=3000,algorithm = "tabu")
print(Bass)

```


```{r}
set.seed(4)
A1<-c(.8,0,0,0)
B1<-c(.3,.8,0,0)
C1<-c(0,0,.8,0)
D1<-c(0,.5,0,.8)
M3<-matrix(rbind(A1,B1,C1,D1),nrow=4, ncol=4)

A<-Sample(N=10000,P=.9,sh=3)
B<-Sample(N=10000,P=.9,sh=.5)
C<-Sample(N=10000,P=.9,sh=.5)
D<-Sample(N=10000,P=.9,sh=0.2)

dat3<-as.data.frame(cbind(A,B,C,D))
names(dat3)<-c("A","B","C","D")
dat_new<-combine_linsteps(c(5,10,15,20),M3,dat3)

BN<-tabu(dat_new)
plot(BN)
Bass<-boot.strength(dat_new,R=500,m=3000,algorithm = "tabu")
print(Bass)

#determinant non-zero, maybe unitary (norm 1)
```



```{r}
#gen matrice instead
a<-gen_matrice(5)
a
dat<-create_net(a,10000)

```

```{r}
dat_lin<-combine_linsteps(c(5,10,15,20),mat,dat_ex1)

BN<-tabu(dat_lin)
plot(BN)
Bass<-boot.strength(d,R=500,m=3000,algorithm = "tabu")
print(Bass)

```

